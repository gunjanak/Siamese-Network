{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1AIGkllXAc5EuVyAMDb_nQH2BJgUjZfm6",
      "authorship_tag": "ABX9TyPE8Y8RHCb//49icifc6cp/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gunjanak/Siamese-Network/blob/main/coversongs_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load"
      ],
      "metadata": {
        "id": "ohwpVEx7NuUZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xR29I75dF9Mm"
      },
      "outputs": [],
      "source": [
        "folder_location = '/content/drive/MyDrive/Colab Notebooks/covers32k'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchaudio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ge2MbQ7SKO0z",
        "outputId": "7fd1f9ee-eb00-422b-8044-2e65fadedeb3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torchaudio) (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0->torchaudio) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->torchaudio) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import torchaudio\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "oOzbzcCFGX5m"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SiameseDataset(Dataset):\n",
        "  def __init__(self,root_folder,sample_rate=22050,waveform_length=16000):\n",
        "    self.root_folder = root_folder\n",
        "    self.sample_rate = sample_rate\n",
        "    self.waveform_length = waveform_length\n",
        "    self.folders = os.listdir(root_folder)\n",
        "    self.triplets = self.generate_triplets()\n",
        "\n",
        "  def generate_triplets(self):\n",
        "    triplets = []\n",
        "    for folder in self.folders:\n",
        "      folder_path = os.path.join(self.root_folder,folder)\n",
        "      mp3_files = [f for f in os.listdir(folder_path) if f.endswith('.mp3')]\n",
        "\n",
        "      #Create positive pairs\n",
        "      for i in range(len(mp3_files)):\n",
        "        anchor = mp3_files[i]\n",
        "        positive = mp3_files[(i+1) % len(mp3_files)]\n",
        "\n",
        "        #Create negative pairs\n",
        "        other_folders = [f for f in self.folders if f != folder]\n",
        "        random_folder = random.choice(other_folders)\n",
        "        random_negative = random.choice(os.listdir(os.path.join(self.root_folder,random_folder)))\n",
        "        triplets.append((os.path.join(folder_path,anchor),\n",
        "                         os.path.join(folder_path,positive),\n",
        "                         os.path.join(self.root_folder,random_folder,random_negative)))\n",
        "    return triplets\n",
        "\n",
        "  def load_audio_file(self,file_path):\n",
        "    waveform,sample_rate = torchaudio.load(file_path,normalize=True)\n",
        "    return waveform, sample_rate\n",
        "\n",
        "  def resize_waveform(self,waveform,length):\n",
        "    if waveform.size(1) < length:\n",
        "      #pad if the waveform is shorter than the specified length\n",
        "      pad_length = length - waveform.size(1)\n",
        "      waveform = F.pad(waveform,(0,pad_length))\n",
        "    elif waveform.size(1) > length:\n",
        "      #Randomly crop if the waveform is longet than the specified length\n",
        "      start = random.randint(0,waveform.size(1)-length)\n",
        "      waveform = waveform[:,start:start+length]\n",
        "    return waveform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.triplets)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    anchor_path,positive_path,negative_path = self.triplets[index]\n",
        "\n",
        "    #Load audio data and convert to tensors\n",
        "    anchor_waveform, _ = self.load_audio_file(anchor_path)\n",
        "    positive_waveform, _ = self.load_audio_file(positive_path)\n",
        "    negative_waveform, _ = self.load_audio_file(negative_path)\n",
        "\n",
        "    anchor_waveform = self.resize_waveform(anchor_waveform,self.waveform_length)\n",
        "    positive_waveform = self.resize_waveform(positive_waveform,self.waveform_length)\n",
        "    negative_waveform = self.resize_waveform(negative_waveform,self.waveform_length)\n",
        "\n",
        "    #again resize from (1,160000) to (1,400,400)\n",
        "    anchor_waveform = anchor_waveform.resize_(1,200,200)\n",
        "    positive_waveform = positive_waveform.resize_(1,200,200)\n",
        "    negative_waveform = negative_waveform.resize_(1,200,200)\n",
        "\n",
        "    return anchor_waveform, positive_waveform,negative_waveform"
      ],
      "metadata": {
        "id": "T04CQp5nGjMJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simaese_dataset = SiameseDataset(folder_location)"
      ],
      "metadata": {
        "id": "X7Sqsrc_Ikuc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anchor,positive,negative = simaese_dataset[0]"
      ],
      "metadata": {
        "id": "SPjIdi93IvJS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anchor.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4A7r2aKhIKS6",
        "outputId": "b5b9ef5c-e3f6-43b8-ff75-f6a1a4de9af6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 200, 200])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "negative.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZKY7m2fL6Ts",
        "outputId": "4281ea77-f99e-4aea-a194-2b702ec29a92"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 200, 200])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "positive.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrcUnCzoL8oP",
        "outputId": "60aa87d6-da87-4e5d-df52-210a8c41bdf3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 200, 200])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SiameseNetwork"
      ],
      "metadata": {
        "id": "HjF4KfjGV9Hd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "_nksh61MWVfN"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SiameseNetwork(nn.Module):\n",
        "  def __init__(self,dropout_prob=0.5):\n",
        "    super(SiameseNetwork,self).__init__()\n",
        "\n",
        "    #Shared convolutional layers\n",
        "    self.convolutional_layers = nn.Sequential(\n",
        "        nn.Conv2d(1,64,kernel_size=3,padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Dropout2d(p=dropout_prob),\n",
        "        nn.MaxPool2d(kernel_size=2,stride=2),\n",
        "        nn.Conv2d(64,128,kernel_size=3,padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Dropout2d(p=dropout_prob),\n",
        "        nn.MaxPool2d(kernel_size=2,stride=2),\n",
        "        nn.Conv2d(128,256,kernel_size=3,padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Dropout2d(p=dropout_prob),\n",
        "        nn.MaxPool2d(kernel_size=2,stride=2)\n",
        "\n",
        "    )\n",
        "\n",
        "    #Fully connected layers for each branch\n",
        "    self.fc_layers = nn.Sequential(\n",
        "        nn.Linear(160000,1024),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Linear(1024,512),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Linear(512,256),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Linear(256,128)\n",
        "    )\n",
        "\n",
        "  def forward_one(self,x):\n",
        "    #forward pass for one branch\n",
        "    x = self.convolutional_layers(x)\n",
        "    # print(x.shape)\n",
        "    x = x.view(x.size()[0],-1)\n",
        "    # print(x.shape)\n",
        "    x = self.fc_layers(x)\n",
        "    return x\n",
        "\n",
        "  def forward(self,anchor,positive,negative):\n",
        "    #forward pass for each branch\n",
        "    anchor_out = self.forward_one(anchor)\n",
        "    positive_out = self.forward_one(positive)\n",
        "    negative_out = self.forward_one(negative)\n",
        "\n",
        "    return anchor_out, positive_out, negative_out\n",
        "\n"
      ],
      "metadata": {
        "id": "mDNbkV_7QBJL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loss function"
      ],
      "metadata": {
        "id": "WTtmx8iZAuDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define triplet loss\n",
        "class TripletLoss(nn.Module):\n",
        "    def __init__(self, margin=1.0):\n",
        "        super(TripletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, anchor, positive, negative):\n",
        "        distance_positive = (anchor - positive).pow(2).sum(1)\n",
        "        distance_negative = (anchor - negative).pow(2).sum(1)\n",
        "        loss = torch.relu(distance_positive - distance_negative + self.margin)\n",
        "        return loss.mean()"
      ],
      "metadata": {
        "id": "WAchZ7a2Atg-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "triplet_loss = TripletLoss()"
      ],
      "metadata": {
        "id": "IPQsKDLFA_Ri"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train"
      ],
      "metadata": {
        "id": "woiCgIjIay45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "i5WYkL7jakcH"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "7XR2lTOHa9BK"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create an instance of the SiameseNetwork\n",
        "dropout_prob = 0.5  # Adjust as needed\n",
        "siamese_net = SiameseNetwork(dropout_prob).to(device)\n",
        "# siamese_net = SiameseNetwork().to(device)"
      ],
      "metadata": {
        "id": "wT3FX1EobGH2"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #define the triplet loss\n",
        "# triplet_loss = nn.TripletMarginLoss(margin=1.0)"
      ],
      "metadata": {
        "id": "M6vllqV5bPM2"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create optimizer\n",
        "optimizer = optim.Adam(siamese_net.parameters(),lr=0.001)"
      ],
      "metadata": {
        "id": "dtL7anRHbV6h"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(simaese_dataset,batch_size=64,shuffle=True,num_workers=2)"
      ],
      "metadata": {
        "id": "5hw8o4f4beZy"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 50"
      ],
      "metadata": {
        "id": "OUfgKwNYbzUN"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Main training loop"
      ],
      "metadata": {
        "id": "LI5agj69bxR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "  total_loss = 0.0\n",
        "\n",
        "  for batch in dataloader:\n",
        "    anchor,positive,negative = batch\n",
        "    anchor,positive,negative = anchor.to(device),positive.to(device),negative.to(device)\n",
        "\n",
        "    #Zero the gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    #Forward pass\n",
        "    anchor_out,positive_out,negative_out = siamese_net(anchor,positive,negative)\n",
        "\n",
        "    #Compute triplet loss\n",
        "    loss = triplet_loss(anchor_out,positive_out,negative_out)\n",
        "\n",
        "    #Backward pass and optimization\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "  #print average loss for the epoch\n",
        "  avg_loss = total_loss / len(dataloader)\n",
        "  print(f\"Epoch {epoch +1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e5N52bzbsPn",
        "outputId": "27b9e4af-13a7-4d98-804c-64af685fd4f0"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Average Loss: 1.3641\n",
            "Epoch 2/50, Average Loss: 1.1888\n",
            "Epoch 3/50, Average Loss: 1.1002\n",
            "Epoch 4/50, Average Loss: 1.0865\n",
            "Epoch 5/50, Average Loss: 1.0161\n",
            "Epoch 6/50, Average Loss: 0.9764\n",
            "Epoch 7/50, Average Loss: 1.0817\n",
            "Epoch 8/50, Average Loss: 1.0156\n",
            "Epoch 9/50, Average Loss: nan\n",
            "Epoch 10/50, Average Loss: 1.0000\n",
            "Epoch 11/50, Average Loss: 1.0000\n",
            "Epoch 12/50, Average Loss: 1.0000\n",
            "Epoch 13/50, Average Loss: 1.0000\n",
            "Epoch 14/50, Average Loss: 1.0000\n",
            "Epoch 15/50, Average Loss: 1.0000\n",
            "Epoch 16/50, Average Loss: 1.0000\n",
            "Epoch 17/50, Average Loss: 1.0000\n",
            "Epoch 18/50, Average Loss: 1.0000\n",
            "Epoch 19/50, Average Loss: 1.0000\n",
            "Epoch 20/50, Average Loss: 1.0000\n",
            "Epoch 21/50, Average Loss: 1.0000\n",
            "Epoch 22/50, Average Loss: 1.0000\n",
            "Epoch 23/50, Average Loss: 1.0000\n",
            "Epoch 24/50, Average Loss: 1.0000\n",
            "Epoch 25/50, Average Loss: 1.0000\n",
            "Epoch 26/50, Average Loss: 1.0000\n",
            "Epoch 27/50, Average Loss: 1.0000\n",
            "Epoch 28/50, Average Loss: 1.0000\n",
            "Epoch 29/50, Average Loss: 1.0000\n",
            "Epoch 30/50, Average Loss: 1.0000\n",
            "Epoch 31/50, Average Loss: 1.0000\n",
            "Epoch 32/50, Average Loss: 1.0000\n",
            "Epoch 33/50, Average Loss: 1.0000\n",
            "Epoch 34/50, Average Loss: 1.0000\n",
            "Epoch 35/50, Average Loss: 1.0000\n",
            "Epoch 36/50, Average Loss: 1.0000\n",
            "Epoch 37/50, Average Loss: 1.0000\n",
            "Epoch 38/50, Average Loss: 1.0000\n",
            "Epoch 39/50, Average Loss: 1.0000\n",
            "Epoch 40/50, Average Loss: 1.0000\n",
            "Epoch 41/50, Average Loss: 1.0000\n",
            "Epoch 42/50, Average Loss: 1.0000\n",
            "Epoch 43/50, Average Loss: 1.0000\n",
            "Epoch 44/50, Average Loss: 1.0000\n",
            "Epoch 45/50, Average Loss: 1.0000\n",
            "Epoch 46/50, Average Loss: 1.0000\n",
            "Epoch 47/50, Average Loss: 1.0000\n",
            "Epoch 48/50, Average Loss: 1.0000\n",
            "Epoch 49/50, Average Loss: 1.0000\n",
            "Epoch 50/50, Average Loss: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4vWJWmkEdAF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PmV448GXayZN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
